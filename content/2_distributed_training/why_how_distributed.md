---
title: "2.1 Why distributed training "
weight: 1
---


![](/images/training/training21.png)

***

![](/images/training/training22.png)

***

![](/images/training/training23.png)

## PyTorch supports several different distributed training backends that perform the All-Reduce operation
